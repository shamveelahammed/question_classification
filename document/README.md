# question_classification

## WordEmbeddingLoader
### HOW TO USE
* GloVe
```python
from torch import LongTensor
from WordEmbeddingLoader import WordEmbeddingLoader

# freeze is by default False, so only needs specifying when true.
word_to_index, embedding = WordEmbeddingLoader.load(freeze=True) 
index =  LongTensor([word_to_index['name']])
print(embedding(index))
```
```bash
tensor([[ 1.1326e-01, -2.3055e-01,  4.6840e-01, -2.6068e-01,  1.2871e-01,
          3.8373e-01, -3.2314e-02, -5.7986e-01,  1.8424e-01,  4.6796e-02,
         -5.5893e-01,  2.7794e-01,  7.4838e-01,  3.3575e-01,  2.6834e-02,
         -4.4505e-01,  1.4755e+00, -1.4081e-01, -3.1658e-01,  5.7686e-01,
         -1.8440e-01, -1.0370e-01, -2.2519e-01,  3.4614e-01, -2.1930e-01,
         -2.3868e-01, -1.2845e-01, -7.7772e-01,  1.6957e-01,  2.9432e-02,
          4.7705e-01,  8.5881e-01, -5.3617e-02,  1.0770e-01,  9.6440e-02,
          2.7325e-01,  4.4933e-02,  1.3310e-03,  5.0395e-02, -4.8147e-01,
          1.7561e-01,  1.9419e-01,  5.1495e-01, -5.6149e-01,  1.1211e-01,
          2.5591e-01,  2.9011e-02, -3.4461e-03, -1.8478e-01, -2.0650e-01,
          1.0358e-01,  7.5532e-01,  9.4688e-01,  8.4278e-01, -7.0051e-01,
         -2.4433e+00, -9.1913e-01, -1.0451e-01,  9.0954e-01,  2.5491e-01,
          2.1129e-01,  1.2046e+00, -9.1615e-02,  3.0364e-01,  1.3768e+00,
         -5.8152e-01,  3.8085e-01,  1.5504e-01,  2.0049e-01,  7.3361e-04,
         -6.2300e-01,  2.0244e-01, -3.0387e-01, -8.1412e-01,  3.8805e-01,
          2.1271e-01, -4.1525e-02, -4.5596e-02, -1.1508e+00, -2.5826e-01,
         -8.9721e-02, -1.1256e+00, -2.5124e-01, -2.8010e-01, -1.0334e+00,
         -1.6813e-01, -4.0975e-01, -1.0685e+00,  7.4311e-01,  8.3244e-02,
         -3.3616e-01, -8.8150e-02,  1.5401e-01,  4.7736e-01, -1.8272e-01,
         -2.5543e-01, -8.9365e-01, -4.6822e-01,  1.9834e-01, -4.8772e-02]],
       grad_fn=<EmbeddingBackward>)
```
* Random
```python
from torch import LongTensor
from WordEmbeddingLoader import WordEmbeddingLoader

# random is by default False - which points to GloVe.
# data_path and frequency_threshold are by default None, as they are not needed for Glove.
word_to_index, embedding = WordEmbeddingLoader.load(random=True, data_path='data\\train_label.txt', frequency_threshold=2)
index = LongTensor([word_to_index['name']])
print(embedding(index))
```
```bash
tensor([[ 3.9767, -3.7362,  1.3508,  5.3946,  6.1296,  7.9175,  4.6109,  9.7723,
          5.1597, -4.6943,  0.4804, -0.8930, -4.8745,  4.9583,  4.7207, -3.5724,
         -4.5541,  8.2829, -4.4200,  2.3837,  7.1392, -7.8685, -9.2198, -3.9581,
         -5.2689,  6.4879, -2.3979,  4.8083, -6.2414,  5.9522,  4.7746, -3.8616,
         -3.2186, -6.1294,  8.4027,  3.4032,  6.0694, -5.7826, -2.9766, -8.0869,
          2.0886,  8.3767,  4.2533, -0.4216,  1.9670, -8.5290, -3.4193, -5.1419,
         -5.9387, -4.3393,  5.2120,  4.2373, -3.9011, -1.4666,  5.8140, -4.5459,
         -7.6593, -0.2991,  8.4109, -1.9186, -4.0094,  6.0459, -5.5141, -1.6606,
         -1.4810,  9.5261, -0.5930,  3.5788, -0.8000, -3.0614,  2.9651,  5.5222,
         -2.4042, -1.3364, -2.7648,  9.9728,  4.3789, -2.1103, -2.0517, -0.1833,
          2.0719, -3.1465,  3.9070, -3.6225, -6.6028, -6.7624, -4.3794,  3.1722,
          0.4343,  9.8298, -4.8722,  9.1777, -8.6320, -3.1353,  6.2808, -9.7483,
         -7.1193, -6.6784, -4.9679,  5.2280]], grad_fn=<EmbeddingBackward>)
```
## BagOfWords
### HOW TO USE
```python
from WordEmbeddingLoader import WordEmbeddingLoader
from BagOfWords import BagOfWords
word_to_index, embedding = WordEmbeddingLoader.load()
bow = BagOfWords(embedding, word_to_index)

vector, label = bow.forward("ENTY:food What do you get by adding Lactobacillus bulgaricus to milk ?")
print(vector)
print(label)
```
```bash
tensor([[-1.8807e-01,  2.7060e-01,  1.4559e-01, -3.8446e-01, -4.4511e-01,
          1.6268e-01, -4.6611e-02,  1.6292e-01,  1.8894e-01, -2.2770e-01,
          9.3888e-03,  4.3767e-02,  1.5394e-01,  6.6573e-02, -5.0421e-02,
         -2.3298e-01,  1.4011e-02,  1.3735e-01, -3.9502e-01,  4.2976e-01,
          2.4106e-01,  6.1968e-03, -5.0587e-02, -1.6007e-01, -1.2037e-02,
          2.8739e-01, -2.9323e-02, -4.5859e-01,  1.8303e-01, -2.0574e-02,
          4.8003e-02,  4.9990e-01, -3.2117e-02,  8.8852e-03,  2.2014e-02,
          3.4009e-01,  9.5243e-02,  3.8049e-02, -1.7309e-02, -2.9080e-01,
         -2.8072e-01, -1.7928e-01, -2.7879e-01, -4.2960e-01, -1.6043e-01,
          1.6936e-01, -1.3036e-01, -3.7004e-01, -2.2629e-01, -7.5640e-01,
         -2.2675e-02,  1.6486e-01, -9.0330e-02,  8.4934e-01, -2.3796e-01,
         -1.7485e+00, -1.0571e-02, -5.9525e-02,  1.1452e+00,  2.1251e-01,
         -8.0536e-02,  5.9981e-01, -3.0237e-01,  5.3585e-03,  7.0007e-01,
          2.6743e-01,  5.6389e-01,  3.0480e-01,  1.1357e-01, -4.5585e-01,
         -3.7189e-02, -1.3849e-01, -1.6728e-03, -2.4724e-01,  1.0204e-01,
          3.2366e-01, -1.5010e-01, -1.3200e-01, -5.3180e-01,  4.6464e-02,
          4.1398e-01, -1.3672e-01, -5.7774e-01,  5.6923e-02, -1.0517e+00,
         -8.7332e-02,  1.1316e-01, -3.8766e-02, -3.6268e-01, -2.6791e-01,
         -2.8019e-02, -2.1037e-01, -2.4737e-01, -3.1128e-01, -3.0808e-01,
         -2.6708e-01, -1.3435e-01, -2.9563e-01,  1.7983e-01,  2.3759e-01]],
       grad_fn=<DivBackward0>)
ENTY:food
```